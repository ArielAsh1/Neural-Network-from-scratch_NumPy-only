      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 - Neural Network using NumPy"
      ],
      "metadata": {
        "id": "M7KcfDUSQ6hB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part we learn to:\n",
        "1. Load and preprocess datasets.\n",
        "2. Implement and train a neural network (multi-layer perceptron) for handwriting recognition (MNIST dataset), using numpy only.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0PDvBctcT5j_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Dataset**\n"
      ],
      "metadata": {
        "id": "QebhdAoMTZxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import useful packages"
      ],
      "metadata": {
        "id": "lGPgtwcbYEEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml"
      ],
      "metadata": {
        "id": "QCxElW-dYHPG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the MNIST dataset "
      ],
      "metadata": {
        "id": "AahLMywDUTaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = fetch_openml('mnist_784', return_X_y=True, as_frame=False)\n",
        "print(X.shape, y.shape)\n",
        "# X- 70000 rows (samples), each with 784 columns (features)\n",
        "# y- 1d vector of 70000 labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvLP-9TtTytH",
        "outputId": "9041a150-4235-4d55-c8c0-bda0cbf2aea8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70000, 784) (70000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DELETE AFTER @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "#print(\"X BEFROE normalize: \\n\", X[1])"
      ],
      "metadata": {
        "id": "_ScH0TWRgqQy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data normalization"
      ],
      "metadata": {
        "id": "nIWk74miWD3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Min-Max normalization of the dataset, so all feature values will be in range [0,1]\n",
        "# rows- 70000. feature values in each row- 784.\n",
        "# for each feature j, calculate this formula on him: (j - min_feature) / (max_feature - min_feature)\n",
        "\n",
        "def min_max_norm(X):\n",
        "  X = np.array(X)\n",
        "# reshaping the vector from a row to a column vector\n",
        "  max_features = np.amax(X,axis=1)[:, np.newaxis] # returns a vector where each value is the max value of its the corresponding column in X\n",
        "  min_features = np.amin(X,axis=1)[:, np.newaxis] # returns a vector where each value is the min value of its the corresponding column in X\n",
        "  \n",
        "  # compute min-max according to formula\n",
        "  normalizedX = (X-min_features)/(max_features-min_features)\n",
        "  return normalizedX\n",
        "\n",
        "X = np.array(min_max_norm(X))\n"
      ],
      "metadata": {
        "id": "oVTy--i4WSnO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DELETE AFTER @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "#print(\"X AFTER normalize: \\n\", X[1])"
      ],
      "metadata": {
        "id": "ZEMgBxQpgmBX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data into Train set and Test set"
      ],
      "metadata": {
        "id": "IGB05L0uW17i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spliting the data into Train set and Test set\n",
        "# train set will be 80% (0.8 * number or rows) and test set will be 20%\n",
        "\n",
        "split_in = int(np.floor(0.8 * X.shape[0]))\n",
        "X_train = X[:split_in]\n",
        "y_train = y[:split_in]\n",
        "\n",
        "X_test = X[split_in:]\n",
        "y_test = y[split_in:]\n"
      ],
      "metadata": {
        "id": "uVDTzcewW-YC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation function"
      ],
      "metadata": {
        "id": "hxQx41j6YXDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the sigmoid activation function and its derivative\n",
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "  return sigmoid(z) * (1 - sigmoid(z))\n"
      ],
      "metadata": {
        "id": "7IHRNTT5Ydcn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax function"
      ],
      "metadata": {
        "id": "bJkyk_WQY1WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns the softmax output of a vector\n",
        "def softmax(z):\n",
        "  exp_z = np.exp(z)\n",
        "  sum = exp_z.sum()\n",
        "  return exp_z / sum\n"
      ],
      "metadata": {
        "id": "K471HopTY6Mu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function"
      ],
      "metadata": {
        "id": "RAlKnQ1-ZJR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Negative Log Likelihood loss function for the multiclass\n",
        "\n",
        "def nll_loss(y_pred, y):\n",
        "  loss = -np.sum(y * np.log(y_pred))\n",
        "  return loss / float(y_pred.shape[0])"
      ],
      "metadata": {
        "id": "of5mjXukZWUy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyper-Parameters"
      ],
      "metadata": {
        "id": "j6BD4CUIXGNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters:\n",
        "EPOCH = 5\n",
        "LEARNING_RATE = 0.25\n",
        "HIDDEN_DIM = 256\n"
      ],
      "metadata": {
        "id": "8faNWU3Ybx-w"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters initialization "
      ],
      "metadata": {
        "id": "PwKZjQ69dapE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing the parameters W1,W2,b1,b2. \n",
        "\n",
        "# init W1 as 2d matrix, with dimensions (HIDDEN_DIM,784), \n",
        "# filled with random values distributed around 0, according to Normal Xavier (sqrt(2/dim_sum))\n",
        "W1 = np.random.normal(0, np.sqrt(2 / (HIDDEN_DIM + 784)),(HIDDEN_DIM,784))\n",
        "\n",
        "# init b as 1d vector filled with zeros, where len=HIDDEN_DIM\n",
        "# and then reshape b1 from row vector (HIDDEN_DIM, ) to column vector (HIDDEN_DIM,1)\n",
        "b1 = np.zeros(HIDDEN_DIM)[:, np.newaxis]\n",
        "\n",
        "# same for W2, b2\n",
        "# W2 dim: (10, HIDDEN_DIM), b2 dim: (10,1)\n",
        "W2 = np.random.normal(0, np.sqrt(2 / (10 +  HIDDEN_DIM)),(10, HIDDEN_DIM))\n",
        "b2 = np.zeros(10)[:, np.newaxis]\n"
      ],
      "metadata": {
        "id": "buCiul3fdiPV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Training**\n"
      ],
      "metadata": {
        "id": "DRhXUPOydoEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X, y, num_of_epochs):  # TODO: why W1,W2,b1,b2 are not recognized from outside \n",
        "  W1 = np.random.normal(0, np.sqrt(2 / (HIDDEN_DIM + 784)),(HIDDEN_DIM,784))\n",
        "  b1 = np.zeros(HIDDEN_DIM)[:, np.newaxis]\n",
        "  W2 = np.random.normal(0, np.sqrt(2 / (10 +  HIDDEN_DIM)),(10, HIDDEN_DIM))\n",
        "  b2 = np.zeros(10)[:, np.newaxis]\n",
        "\n",
        "  train_size = len(X)\n",
        "\n",
        "  for epoch in range(num_of_epochs):\n",
        "    avg_epoch_loss = 0\n",
        "    for i in range(train_size):\n",
        "\n",
        "      # Forward propagation:\n",
        "      # for single x: output = softmax(W2(sigmoid(W1x+b1))+b2)\n",
        "      # loss nnl(output, y_true ) = nnl(softmax(W2(sigmoid(W1x+b1))+b2), y_true)\n",
        "      \n",
        "      x = X[i]  # single row vector (784,)\n",
        "      x = x[:, np.newaxis]  # reshape x from a row vector to column vector (784,1)\n",
        "\n",
        "      # W1 (HIDDEN_DIM,784) , x (784,1) --> W1x (HIDDEN_DIM,1)\n",
        "      # b1 (HIDDEN_DIM,1) --> W1x +b1 = z1 (HIDDEN_DIM,1)\n",
        "      z1 = W1.dot(x) + b1\n",
        "      h1 = sigmoid(z1)\n",
        "\n",
        "      # W2 (10,HIDDEN_DIM) , h1 (HIDDEN_DIM,1) --> W2h1 (10,1)\n",
        "      # b2 (10,1) --> W2h1 +b2 = Z2 (10,1)\n",
        "      Z2 = W2.dot(h1) + b2\n",
        "      # y_hat remains vector (10,1), but filled now with probabilities\n",
        "      y_hat = softmax(Z2)\n",
        "      \n",
        "      # one-hot encoding:\n",
        "      # create a one_hot vetor to be used as y_true. one_hot is a column of zeros (10,1)\n",
        "      one_hot = np.zeros(10)[:, np.newaxis]\n",
        "      # assign 1 according to current class, so all rows are 0 and we have 1 in the y[i] place\n",
        "      one_hot[int(y[i])] = 1\n",
        "      y_true = one_hot\n",
        "\n",
        "      # Compute the loss:\n",
        "      loss = nll_loss(y_hat, y_true)\n",
        "      avg_epoch_loss = avg_epoch_loss + loss\n",
        "\n",
        "      # Back propagation - compute the gradients of each parameter:\n",
        "      # y_hat (10,1) , y_true (10,1) --> dZ2 (10,1)\n",
        "      dZ2 = (y_hat - y_true)\n",
        "      # dZ2 (10,1) , h1.T (1,HIDDEN_DIM) --> dW2 (10,HIDDEN_DIM)\n",
        "      dW2 = dZ2.dot(h1.T)  # T to reshape as a column vector (1,HIDDEN_DIM)\n",
        "      db2 = dZ2\n",
        "\n",
        "      # W2.T (HIDDEN_DIM,10) , dZ2 (10,1) --> dh1 (HIDDEN_DIM, 1)\n",
        "      dh1 = W2.T.dot(dZ2)\n",
        "      # here for dz1 we perform elementwise multiplication\n",
        "      # dh1 (HIDDEN_DIM, 1) , z1 (HIDDEN_DIM,1) --> dz1 (HIDDEN_DIM, 1)\n",
        "      dz1 = dh1 * sigmoid_derivative(z1)\n",
        "      # dz1 (HIDDEN_DIM, 1) , x.T (1,784) --> dW1 (HIDDEN_DIM, 784)\n",
        "      dW1 = dz1.dot(x.T)\n",
        "      db1 = dz1\n",
        "\n",
        "      # Update weights:\n",
        "      W2 = W2 - LEARNING_RATE * dW2\n",
        "      b2 = b2 - LEARNING_RATE * db2\n",
        "      W1 = W1 - LEARNING_RATE * dW1\n",
        "      b1 = b1 - LEARNING_RATE * db1\n",
        "\n",
        "      # for testing:\n",
        "      # if i%10000 == 0:\n",
        "      #   print(i, avg_epoch_loss/i)\n",
        "    \n",
        "    avg_epoch_loss = (avg_epoch_loss/train_size) \n",
        "    \n",
        "    print(\"Epoch:\", epoch,\" Loss:\", avg_epoch_loss)\n",
        "  return W1, W2, b1, b2"
      ],
      "metadata": {
        "id": "atqyODvneEG_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Test**"
      ],
      "metadata": {
        "id": "trwxFsfdhkAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you should test your model"
      ],
      "metadata": {
        "id": "8Soln0ZviJJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the model and return the accuracy on the test set\n",
        "def test(X, y, W1, W2, b1, b2):\n",
        "  true_pred_counter = 0\n",
        "  for i in range(len(X)):\n",
        "    x = X[i]\n",
        "    x = x[:, np.newaxis]  # x (784,1)\n",
        "    z1 = W1.dot(x) + b1 # z1 (HIDDEN_DIM, 1)\n",
        "    h1 = sigmoid(z1)\n",
        "    Z2 = W2.dot(h1) + b2  # Z2 (10,1)\n",
        "    y_hat = softmax(Z2)\n",
        "    predicted = np.argmax(y_hat)\n",
        "    if float(predicted) == float(y[i]):      \n",
        "      true_pred_counter += 1\n",
        "      \n",
        "  accuracy = float(true_pred_counter) / len(X)\n",
        "  return accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "uP1PX2xnhnoc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Main**"
      ],
      "metadata": {
        "id": "L52uxH7RjJ25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "W1, W2, b1, b2 = train(X_train, y_train, EPOCH)\n",
        "accuracy = test(X_test, y_test, W1, W2, b1, b2)\n",
        "\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "jA5UbzSxjjcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bce80f84-44d1-4e0e-fb7e-d0585ef242ec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0  Loss: 0.03334529281463319\n",
            "Epoch: 1  Loss: 0.024500722722878676\n",
            "Epoch: 2  Loss: 0.02200117945967143\n",
            "Epoch: 3  Loss: 0.02336657229421015\n",
            "Epoch: 4  Loss: 0.021053658124992623\n",
            "0.9355714285714286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
     
